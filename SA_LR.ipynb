{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQT-JNHAdYs"
      },
      "source": [
        "# Import libraries\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "# Define a class to store a single sentiment example\n",
        "class SentimentExample:\n",
        "    def __init__(self, words, label):\n",
        "        self.words = words\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "\n",
        "# Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences.\n",
        "def read_sentiment_examples(infile):\n",
        "    f = open(infile, encoding='iso8859')\n",
        "    exs = []\n",
        "    for line in f:\n",
        "            fields = line.strip().split(\" \")\n",
        "            label = 0 if \"0\" in fields[0] else 1\n",
        "            exs.append(SentimentExample(fields[1:], label))\n",
        "    f.close()\n",
        "    return exs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsuUSp74D8xE"
      },
      "source": [
        "#### *Load* the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rD7HHnCEDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29cc33c5-b680-40fc-b97e-662198774cb6"
      },
      "source": [
        "#Mount drive to access files in gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN_xgUFDCJ7x"
      },
      "source": [
        "#YOU NEED TO CHANGE PATH OF train_file AND dev_file ACCORDING TO WHERE YOU STORE THEM IN YOUR gdrive.\n",
        "#You can also check the video of Week 2 tutorial (May 17th Zoom recoding) by Punar (GA) where he explains how to do it.\n",
        "\n",
        "#\"TODO\" change path for train_file\n",
        "train_file = '/content/gdrive/MyDrive/Assignment 1/Data/train.txt'\n",
        "#\"TODO\" change path for dev_file\n",
        "dev_file = '/content/gdrive/MyDrive/Assignment 1/Data/dev.txt'\n",
        "\n",
        "# Load the data from the files\n",
        "train_exs = read_sentiment_examples(train_file)\n",
        "dev_exs = read_sentiment_examples(dev_file)\n",
        "n_pos = 0\n",
        "n_neg = 0\n",
        "for ex in train_exs:\n",
        "    if ex.label == 1:\n",
        "        n_pos += 1\n",
        "    else:\n",
        "        n_neg += 1\n",
        "print(\"%d train examples: %d positive, %d negative\" % (len(train_exs), n_pos, n_neg))\n",
        "print(\"%d dev examples\" % len(dev_exs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9xymaLCfwd"
      },
      "source": [
        "# Bijection between objects and integers starting at 0. Useful for mapping\n",
        "# labels, features, etc. into coordinates of a vector space.\n",
        "\n",
        "# This class creates a mapping between objects (here words) and unique indices\n",
        "# For example: apple->1, banana->2, and so on\n",
        "class Indexer(object):\n",
        "    def __init__(self):\n",
        "        self.objs_to_ints = {}\n",
        "        self.ints_to_objs = {}\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.objs_to_ints)\n",
        "\n",
        "    # Returns the object corresponding to the particular index\n",
        "    def get_object(self, index):\n",
        "        if (index not in self.ints_to_objs):\n",
        "            return None\n",
        "        else:\n",
        "            return self.ints_to_objs[index]\n",
        "\n",
        "    def contains(self, object):\n",
        "        return self.index_of(object) != -1\n",
        "\n",
        "    # Returns -1 if the object isn't present, index otherwise\n",
        "    def index_of(self, object):\n",
        "        if (object not in self.objs_to_ints):\n",
        "            return -1\n",
        "        else:\n",
        "            return self.objs_to_ints[object]\n",
        "\n",
        "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
        "    def add_and_get_index(self, object, add=True):\n",
        "        if not add:\n",
        "            return self.index_of(object)\n",
        "        if (object not in self.objs_to_ints):\n",
        "            new_idx = len(self.objs_to_ints)\n",
        "            self.objs_to_ints[object] = new_idx\n",
        "            self.ints_to_objs[new_idx] = object\n",
        "        return self.objs_to_ints[object]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6gqlXcTCnRg"
      },
      "source": [
        "### Define Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oFLhbSRCmHB"
      },
      "source": [
        "# Import libraries\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fw9mU0bCwDX"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Feature extraction base type. Takes an example and returns an indexed list of features.\n",
        "class FeatureExtractor(object):\n",
        "    # Extract features. Includes a flag add_to_indexer to control whether the indexer should be expanded.\n",
        "    # At test time, any unseen features should be discarded, but at train time, we probably want to keep growing it.\n",
        "    def extract_features(self, ex, add_to_indexer):\n",
        "        raise Exception(\"Don't call me, call my subclasses\")\n",
        "\n",
        "\n",
        "# Extracts unigram bag-of-words features from a sentence. It's up to you to decide how you want to handle counts\n",
        "class UnigramFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, indexer: Indexer):\n",
        "        self.indexer = indexer\n",
        "        self.sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def extract_features(self, ex, add_to_indexer=False):\n",
        "        features = Counter()\n",
        "        for w in ex.words:\n",
        "            if (self.sid.polarity_scores(w)['compound'] >= 0.5) or (self.sid.polarity_scores(w)['compound'] <= -0.5):\n",
        "                feat_idx = self.indexer.add_and_get_index(w) if add_to_indexer else self.indexer.index_of(w)\n",
        "                if feat_idx != -1:\n",
        "                    features[feat_idx] += 1.0\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBmYcjXUC4wD"
      },
      "source": [
        "# Sentiment classifier base type\n",
        "class SentimentClassifier(object):\n",
        "    # Makes a prediction for the given\n",
        "    def predict(self, ex: SentimentExample):\n",
        "        raise Exception(\"Don't call me, call my subclasses\")\n",
        "\n",
        "\n",
        "# Always predicts the positive class\n",
        "class AlwaysPositiveClassifier(SentimentClassifier):\n",
        "    def predict(self, ex: SentimentExample):\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRwt0oS0C-i7"
      },
      "source": [
        "class LogisticRegressionClassifier(SentimentClassifier):\n",
        "    def __init__(self, train_exs, feat_extractor: FeatureExtractor, num_iters=50, reg_lambda=0.0, learning_rate=0.2):\n",
        "        # TODO: Initialize the logistic regression model\n",
        "        \n",
        "        # Arguments: feat_extractor is unigram, train_examples is train dataset\n",
        "        # num_iters is the number of epochs, reg_lambda is the regularization parameter\n",
        "        # learning_rate is the learning rate used in gradient descent\n",
        "        \n",
        "        # STEP 1: Define variables for weights and biases, and initialize them\n",
        "        \n",
        "        # STEP 2: Call the train() function. (This has already been done for you)\n",
        "        \n",
        "        ##### SOLUTION START #####\n",
        "        self.feat_extractor = feat_extractor\n",
        "        self.feature_size = len(feat_extractor.indexer.objs_to_ints)\n",
        "\n",
        "        # self.w = np.random.randn(1, self.feature_size)\n",
        "        self.w = np.zeros([1, self.feature_size])\n",
        "        self.b = .1\n",
        "\n",
        "        ##### SOLUTION END #####\n",
        "\n",
        "        self.train(train_exs, num_iters, reg_lambda, learning_rate)\n",
        "\n",
        "\n",
        "    def train(self, train_exs, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
        "        # TODO: Function for training the logistic regression model\n",
        "        \n",
        "        # STEP 1: Write a 'for' loop which iterates over the dataset num_iters times\n",
        "        \n",
        "        # STEP 2: Write an inner 'for' loop for each step of gradient descent\n",
        "        # Use a stochastic gradient descent\n",
        "        \n",
        "        # STEP 3: In each step of gradient descent apply the update rule\n",
        "        # to weights and biases\n",
        "\n",
        "        ##### SOLUTION START #####\n",
        "        \n",
        "        for itr in range(num_iters):\n",
        "            losses = []\n",
        "            for ex in train_exs:\n",
        "                y_hat, x = self.predict(ex)\n",
        "                y = ex.label\n",
        "                loss = (y - y_hat)*(y - y_hat)\n",
        "                dw = 2 * (y_hat - y)* np.transpose(x)\n",
        "                db = (y_hat - y)\n",
        "                self.w = self.w - learning_rate * dw\n",
        "                self.b = self.b - learning_rate * db\n",
        "                losses.append(loss)\n",
        "            print ('Iteration : {0}, Loss : {1}'.format(itr, np.mean(losses)))\n",
        "\n",
        "        ##### SOLUTION END #####\n",
        "\n",
        "    def predict(self, ex):\n",
        "        # TODO: Logistic regression model's prediction for a single example\n",
        "        ##### SOLUTION START #####\n",
        "        feature = self.feat_extractor.extract_features(ex, False)\n",
        "        x = np.zeros([self.feature_size, 1])\n",
        "        for (k, v) in feature.items():\n",
        "            x[k] = v\n",
        "\n",
        "        # x = (x-np.min(x)) / (np.max(x) - np.min(x))\n",
        "        o = self.w.dot(x) + self.b\n",
        "        return 1.0/(1.0 + np.exp(-o)), x\n",
        "        ##### SOLUTION END #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq7EviDzDJvh"
      },
      "source": [
        "#### Training function for logistic regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_2SxAgDKfd"
      },
      "source": [
        "# Train a logsitic regression model on the given training examples using the given FeatureExtractor\n",
        "def train_lr(train_exs, feat_extractor: FeatureExtractor, reg_lambda):\n",
        "    # TODO: Function for training logistic regression model.\n",
        "    # Populate the feature_extractor.\n",
        "    # Initialize and return an object of instance LogisticRegressionClassifier\n",
        "    \n",
        "    ##### SOLUTION START #####\n",
        "    for tx in train_exs:\n",
        "        feat_extractor.extract_features(tx, True)\n",
        "\n",
        "    return LogisticRegressionClassifier(train_exs, feat_extractor, num_iters=500, learning_rate=0.02)\n",
        "    ##### SOLUTION END #####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTeBU9hmDRKj"
      },
      "source": [
        "# Main entry point for your modifications. Trains and returns one of several models depending on the options passed\n",
        "def train_model(feature_type, model_type, train_exs, reg_lambda=0.0):\n",
        "    \n",
        "    # Initialize feature extractor\n",
        "    if feature_type == \"unigram\":\n",
        "        # Add additional preprocessing code here\n",
        "        feat_extractor = UnigramFeatureExtractor(Indexer())\n",
        "    else:\n",
        "        raise Exception(\"Pass unigram\")\n",
        "\n",
        "    # Train the model\n",
        "    if model_type == \"AlwaysPositive\":\n",
        "        model = AlwaysPositiveClassifier()\n",
        "    elif model_type == \"LogisticRegression\":\n",
        "        model = train_lr(train_exs, feat_extractor, reg_lambda=reg_lambda)\n",
        "    else:\n",
        "        raise Exception(\"Pass AlwaysPositive or LogisticRegression\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP5n47-tDUki"
      },
      "source": [
        "### Functions for evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGPZQt60DVZO"
      },
      "source": [
        "# Evaluates a given classifier on the given examples\n",
        "def evaluate(classifier, exs):\n",
        "    return print_evaluation([ex.label for ex in exs], [classifier.predict(ex)[0] for ex in exs])\n",
        "\n",
        "\n",
        "# Prints accuracy comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
        "def print_evaluation(golds, predictions):\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] >= 0.5:\n",
        "            predictions[i] = 1\n",
        "        else:\n",
        "            predictions[i] = 0\n",
        "    print(predictions)\n",
        "    print(golds)\n",
        "    num_correct = 0\n",
        "    num_pos_correct = 0\n",
        "    num_pred = 0\n",
        "    num_gold = 0\n",
        "    num_total = 0\n",
        "    if len(golds) != len(predictions):\n",
        "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" %\n",
        "                        (len(golds), len(predictions)))\n",
        "    for idx in range(0, len(golds)):\n",
        "        gold = golds[idx]\n",
        "        prediction = predictions[idx]\n",
        "        if prediction == gold:\n",
        "            num_correct += 1\n",
        "        if prediction == 1:\n",
        "            num_pred += 1\n",
        "        if gold == 1:\n",
        "            num_gold += 1\n",
        "        if prediction == 1 and gold == 1:\n",
        "            num_pos_correct += 1\n",
        "        num_total += 1\n",
        "\n",
        "    print(\"Accuracy: %i / %i = %.2f %%\" %\n",
        "          (num_correct, num_total,\n",
        "           num_correct * 100.0 / num_total))\n",
        "    return num_correct * 100.0 / num_total\n",
        "    \n",
        "# Evaluate on train and dev dataset\n",
        "def eval_train_dev(model):\n",
        "    print(\"===== Train Accuracy =====\")\n",
        "    train_acc = evaluate(model, train_exs)\n",
        "    print(\"===== Dev Accuracy =====\")\n",
        "    eval_acc = evaluate(model, dev_exs)\n",
        "    return [train_acc, eval_acc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPAE-pLDa2_"
      },
      "source": [
        "### Evaluating Model with Unigram Bag-of-Words Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNI04BKyDbdO"
      },
      "source": [
        "# Evaluate logistic regression with unigram features\n",
        "unigram_model = train_model('unigram', 'LogisticRegression', train_exs)\n",
        "eval_train_dev(unigram_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}